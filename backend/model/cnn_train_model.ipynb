{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "- The main purpose of this notebook is to process the 1000 .wav samples found in the local Data directory, slice them into smaller audio clips, convert these audio clips into mel-spectrograms, and convert the mel-spectrograms into numpy arrays.\n",
    "- Once the audio data is processed, it can be converted into a dataset. The data set is broken up into training data and testing data. \n",
    "- The training and testing data is then used to train our model, which is then saved in /backend/genre_categorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = '../Data'\n",
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_data(path_name):\n",
    "    \"\"\"Function to create paths/genre arrays to allow for\n",
    "    future conversion of audio files to mel_spectrograms.\"\"\"\n",
    "    paths, genres = [], []\n",
    "    for root, _, files in os.walk(path_name):\n",
    "        for name in files:\n",
    "            filename = os.path.join(root, name)\n",
    "            genre = os.path.split(root)[-1]\n",
    "            paths.append(filename)\n",
    "            genres.append(genre)\n",
    "    return paths, genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_melspectrogram(audio_file, sample_rate):\n",
    "    \"\"\" Function to create a mel_spectrogram from a received audio_file\"\"\"\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(\n",
    "          y=audio_file,\n",
    "          sr=sample_rate,\n",
    "          n_fft=2048,\n",
    "          hop_length=512,\n",
    "          n_mels=128)\n",
    "    # db = decibel units\n",
    "    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "    return mel_spectrogram_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_audio(mel_db_spect, max_length):\n",
    "    \"\"\" Pads mel_spectrogram files to ensure that they are homogeneous\n",
    "    in shape. \"\"\"\n",
    "    if mel_db_spect.shape[1] < max_length:\n",
    "        padding = max_length - mel_db_spect.shape[1]\n",
    "        mel_db_spect = np.pad(mel_db_spect, pad_width=((0,0), (0, padding)), mode='constant')\n",
    "    else:\n",
    "        mel_db_spect = mel_db_spect[:, :max_length]\n",
    "    return mel_db_spect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_songs(audio_paths, genres, max_length=78):\n",
    "    \"\"\" Function that splits received songs into smaller audio clips and\n",
    "    converts them into mel-spectrograms to be used for training/prediction.\n",
    "    Adapted from code found at: \n",
    "    https://github.com/chittalpatel/Music-Genre-Classification-GTZAN/blob/master/Music%20Genre%20Classification/CNN_train(1).ipynb\n",
    "    \"\"\"\n",
    "    split_spects_mel_db = []\n",
    "    split_genres = []\n",
    "    window = 0.06\n",
    "    overlap = 0.3\n",
    "\n",
    "    for path, genre in tqdm(zip(audio_paths, genres), total=len(audio_paths),desc='Processing Audio Files'):  \n",
    "        audio, sample_rate = librosa.load(path)  \n",
    "        audio_shape = audio.shape[0]\n",
    "        chunk = int(audio_shape * window)\n",
    "        offset = int(chunk*(1 - overlap))\n",
    "        individual_split_song = []\n",
    "\n",
    "        # create array of smaller audio clips\n",
    "        for i in range(0, audio_shape - chunk + offset, offset):\n",
    "            individual_split_song.append(audio[i:i+chunk])\n",
    "        \n",
    "        # convert small clips into mel_spectrograms\n",
    "        for sample in individual_split_song:\n",
    "            if sample.shape[0] != chunk:\n",
    "                continue\n",
    "            mel_spec = librosa.feature.melspectrogram(y=sample, sr=sample_rate, n_fft=2048, hop_length=512, n_mels=128)\n",
    "            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            \n",
    "            # Pad outputs to ensure uniformity \n",
    "            mel_spec_db = pad_audio(mel_spec_db, max_length)\n",
    "            \n",
    "            split_spects_mel_db.append(mel_spec_db)          \n",
    "            split_genres.append(genre)\n",
    "    \n",
    "    return split_spects_mel_db, split_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths, gens = get_audio_data(DATA_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Audio Files: 100%|██████████| 1000/1000 [01:51<00:00,  8.98it/s]\n"
     ]
    }
   ],
   "source": [
    "spects, gens2 = split_songs(paths, gens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine size of training set and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spects_len = len(spects)\n",
    "train_size = spects_len * .90\n",
    "test_size = spects_len - train_size\n",
    "train_take = train_size / 50\n",
    "test_take = test_size / 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = np.array(gens2)\n",
    "s1 = np.array(spects)\n",
    "genres_encoded = label_encoder.fit_transform(g1)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((s1, genres_encoded))\n",
    "dataset = dataset.shuffle(len(s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = dataset.cache()\n",
    "final_data = final_data.batch(50)\n",
    "final_data = final_data.prefetch(25)\n",
    "train = final_data.take(int(train_take))\n",
    "test = final_data.skip(int(train_take)).take(int(test_take))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram_height = 128\n",
    "spectrogram_width = 78\n",
    "num_channels = 1  #should be 1 because it is a numpy array, not a color image.\n",
    "num_classes = 10\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(spectrogram_height, spectrogram_width, num_channels)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax') \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "414/414 [==============================] - 48s 113ms/step - loss: 1.6268 - accuracy: 0.4668 - val_loss: 1.1071 - val_accuracy: 0.6096\n",
      "Epoch 2/15\n",
      "414/414 [==============================] - 46s 111ms/step - loss: 1.0035 - accuracy: 0.6536 - val_loss: 0.8490 - val_accuracy: 0.7057\n",
      "Epoch 3/15\n",
      "414/414 [==============================] - 46s 112ms/step - loss: 0.8089 - accuracy: 0.7224 - val_loss: 0.7584 - val_accuracy: 0.7383\n",
      "Epoch 4/15\n",
      "414/414 [==============================] - 46s 111ms/step - loss: 0.7104 - accuracy: 0.7565 - val_loss: 0.7348 - val_accuracy: 0.7470\n",
      "Epoch 5/15\n",
      "414/414 [==============================] - 49s 119ms/step - loss: 0.6325 - accuracy: 0.7822 - val_loss: 0.7304 - val_accuracy: 0.7517\n",
      "Epoch 6/15\n",
      "414/414 [==============================] - 50s 120ms/step - loss: 0.5594 - accuracy: 0.8065 - val_loss: 0.7238 - val_accuracy: 0.7617\n",
      "Epoch 7/15\n",
      "414/414 [==============================] - 50s 121ms/step - loss: 0.5029 - accuracy: 0.8251 - val_loss: 0.7184 - val_accuracy: 0.7670\n",
      "Epoch 8/15\n",
      "414/414 [==============================] - 49s 117ms/step - loss: 0.4464 - accuracy: 0.8442 - val_loss: 0.8441 - val_accuracy: 0.7313\n",
      "Epoch 9/15\n",
      "414/414 [==============================] - 48s 117ms/step - loss: 0.3779 - accuracy: 0.8659 - val_loss: 0.7999 - val_accuracy: 0.7391\n",
      "Epoch 10/15\n",
      "414/414 [==============================] - 48s 116ms/step - loss: 0.3535 - accuracy: 0.8785 - val_loss: 0.9542 - val_accuracy: 0.7074\n",
      "Epoch 11/15\n",
      "414/414 [==============================] - 48s 116ms/step - loss: 0.3274 - accuracy: 0.8822 - val_loss: 1.0245 - val_accuracy: 0.7026\n",
      "Epoch 12/15\n",
      "414/414 [==============================] - 48s 116ms/step - loss: 0.2906 - accuracy: 0.8966 - val_loss: 0.9164 - val_accuracy: 0.7339\n",
      "Epoch 13/15\n",
      "414/414 [==============================] - 49s 118ms/step - loss: 0.2375 - accuracy: 0.9156 - val_loss: 1.0347 - val_accuracy: 0.7283\n",
      "Epoch 14/15\n",
      "414/414 [==============================] - 49s 117ms/step - loss: 0.2043 - accuracy: 0.9268 - val_loss: 1.0251 - val_accuracy: 0.7561\n",
      "Epoch 15/15\n",
      "414/414 [==============================] - 49s 117ms/step - loss: 0.1667 - accuracy: 0.9427 - val_loss: 1.1518 - val_accuracy: 0.7426\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x265e0c31ae0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train, epochs=15, validation_data=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../genre_categorization\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../genre_categorization\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('../genre_categorization')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
