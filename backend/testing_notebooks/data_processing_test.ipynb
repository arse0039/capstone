{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_audio(file_path):\n",
    "    # Load audio file using librosa\n",
    "    audio, sample_rate = librosa.load(file_path, sr=22050)\n",
    "\n",
    "    # Compute Mel spectrogram\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=sample_rate, n_fft=2048, hop_length=512, n_mels=128)\n",
    "    mel_spec_decibel = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "    return mel_spec_decibel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"../Data\"\n",
    "audio_data = []\n",
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_data(path_name):\n",
    "    paths, genres = [], []\n",
    "    for root, _, files in os.walk(path_name):\n",
    "        for name in files:\n",
    "            filename = os.path.join(root, name)\n",
    "            genre = os.path.split(root)[-1]\n",
    "            paths.append(filename)\n",
    "            genres.append(genre)\n",
    "    return paths, genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths, gens = get_audio_data(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_songs(audio_paths, genres, max_length=78):\n",
    "    split_spects_mel_db = []\n",
    "    split_genres = []\n",
    "    window = 0.06\n",
    "    overlap = 0.3\n",
    "\n",
    "    for path, genre in tqdm(zip(audio_paths, genres), total=len(audio_paths),desc='Processing Audio Files'):  \n",
    "        audio, sample_rate = librosa.load(path)  \n",
    "        audio_shape = audio.shape[0]\n",
    "        chunk = int(audio_shape * window)\n",
    "        offset = int(chunk*(1 - overlap))\n",
    "        individual_split_song = []\n",
    "\n",
    "        # create array of smaller audio clips\n",
    "        for i in range(0, audio_shape - chunk + offset, offset):\n",
    "            individual_split_song.append(audio[i:i+chunk])\n",
    "        \n",
    "        # convert small clips into mel_spectrograms\n",
    "        for sample in individual_split_song:\n",
    "            if sample.shape[0] != chunk:\n",
    "                continue\n",
    "            mel_spec = librosa.feature.melspectrogram(y=sample, sr=sample_rate, n_fft=2048, hop_length=512, n_mels=128)\n",
    "            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            \n",
    "            # Pad outputs to ensure uniformity \n",
    "            if mel_spec_db.shape[1] < max_length:\n",
    "                padding = max_length - mel_spec_db.shape[1]\n",
    "                mel_spec_db = np.pad(mel_spec_db, pad_width=((0,0), (0, padding)), mode='constant')\n",
    "            else:\n",
    "                mel_spec_db = mel_spec_db[:, :max_length]\n",
    "            \n",
    "                        \n",
    "            #split_spects_mel_only.append(mel_spec)\n",
    "            split_spects_mel_db.append(mel_spec_db)\n",
    "            \n",
    "            split_genres.append(genre)\n",
    "    \n",
    "    return split_spects_mel_db, split_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Audio Files: 100%|██████████| 1000/1000 [01:52<00:00,  8.90it/s]\n"
     ]
    }
   ],
   "source": [
    "spects, gens2 = split_songs(paths, gens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = len(spects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20700.0 2300.0 414.0 46.0\n"
     ]
    }
   ],
   "source": [
    "tr = l * .90\n",
    "ts = l - tr\n",
    "tr1 = tr / 50\n",
    "ts1 = ts/50\n",
    "print(tr, ts, tr1, ts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = np.array(gens2)\n",
    "s1 = np.array(spects)\n",
    "genres_encoded = label_encoder.fit_transform(g1)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((s1, genres_encoded))\n",
    "dataset = dataset.shuffle(len(s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = dataset.cache()\n",
    "final_data = final_data.batch(50)\n",
    "final_data = final_data.prefetch(25)\n",
    "train = final_data.take(int(tr1))\n",
    "test = final_data.skip(int(tr1)).take(int(ts1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, labels = train.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 78)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram_height = 128\n",
    "spectrogram_width = 78\n",
    "num_channels = 1  #should be 1 because it is a numpy array, not a color image.\n",
    "num_classes = 10\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(spectrogram_height, spectrogram_width, num_channels)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax') #could be sigmoid?\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(train, epochs=15, validation_data=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: genre_categorization\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: genre_categorization\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('genre_categorization')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
